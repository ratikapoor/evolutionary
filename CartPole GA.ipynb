{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2838,
     "status": "ok",
     "timestamp": 1590346067471,
     "user": {
      "displayName": "ritika kapoor",
      "photoUrl": "",
      "userId": "00627906333276749514"
     },
     "user_tz": -330
    },
    "id": "E5u2Xgl45wMh",
    "outputId": "d0c70d16-b863-4b1d-f0c4-46d40de8a42b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import common\n",
    "from common import multiprocess\n",
    "from gym.wrappers import Monitor\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random as rnd\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from gym import envs\n",
    "import datetime\n",
    "import time\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OkIzJgWz73tr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Box2D\n",
      "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Box2D\n",
      "Successfully installed Box2D-2.3.10\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8867,
     "status": "ok",
     "timestamp": 1590346073592,
     "user": {
      "displayName": "ritika kapoor",
      "photoUrl": "",
      "userId": "00627906333276749514"
     },
     "user_tz": -330
    },
    "id": "4iGn5JH4xAP-",
    "outputId": "a56d2e34-69d5-48ce-e5ad-7eb9500764aa"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bwf7Wz39qewz"
   },
   "outputs": [],
   "source": [
    "#parallel environments are created to calculate fitness score \n",
    "num_envs = 10\n",
    "env_name ='CartPole-v1'\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "env= [make_env() for i in range(num_envs)]\n",
    "env= SubprocVecEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "68CA2kSD5wMl"
   },
   "outputs": [],
   "source": [
    "class LunarLander(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(4, 128)\n",
    "        # Output layer, 2 units - for 2 actions\n",
    "        self.output = nn.Linear(128, 2)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "THcWKPzH5wMq"
   },
   "outputs": [],
   "source": [
    "def population(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = LunarLander().to(device)\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        agents.append(agent)\n",
    "                \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n4IzO2GUrFKs"
   },
   "outputs": [],
   "source": [
    "def fitness(agent):\n",
    "    \"\"\"\n",
    "    calculates fitness score of each individual in the population. Mean of scores from 10 parallel environment\n",
    "    are returned\n",
    "    \"\"\"\n",
    "    \n",
    "    agent.eval()\n",
    "    \n",
    "    masks=[]\n",
    "    rewards=[]\n",
    "    state = env.reset()\n",
    "    r=0\n",
    "    m=0\n",
    "    for _ in range(250):\n",
    "      state = torch.FloatTensor(state).to(device)\n",
    "      output_probabilities = agent(state)\n",
    "      action=output_probabilities.multinomial(1).squeeze(1)\n",
    "      next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "      r=reward+r\n",
    "      masks.append(1 - done)\n",
    "      rewards.append(reward)\n",
    "      state=next_state\n",
    "    masks=np.array(masks)\n",
    "    \n",
    "    #For each environment reward is calulated till done =False. Rewards after done=True are ignored.\n",
    "    idx=[np.where(masks[:,i]==0)[0][0] if(len(np.where(masks[:,i]==0)[0])>0) else len(masks) for i in range(masks.shape[1])]\n",
    "    rewards=np.array(rewards)\n",
    "    \n",
    "    #mean of the parrallel environments\n",
    "    r=np.mean([rewards[0:r,c].sum(axis=0) for c,r in enumerate(idx)])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RecScdTB5wM5"
   },
   "outputs": [],
   "source": [
    "def mutate(agent,eps_threshold):\n",
    "    \"\"\"\n",
    "    input: individual to be mutated and threshold by which each weight will be mutated\n",
    "    returns mutated individual\n",
    "    \"\"\"\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = eps_threshold \n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "                                        \n",
    "        if(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    if rnd.random()<0.6:\n",
    "                      param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                if rnd.random()<0.6:\n",
    "                  param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d7cUSovjMIXL"
   },
   "outputs": [],
   "source": [
    "def uniform_crossover(ind1,ind2,layers,uni_prob,mut_threshold):\n",
    "    \"\"\"\n",
    "    input: ind1,ind2 - parents between whom crossover is to be performed\n",
    "           layers - total layers in the neural net * 2(to account for the bias)\n",
    "           uni_prob - probability for uniform crossover\n",
    "           mut_threshold - probability for mutation\n",
    "    returns tweaked copy of parents\n",
    "    \"\"\"\n",
    "    offspring=[]\n",
    "    for i in range(layers):\n",
    "        dim=list(ind1.parameters())[i].shape\n",
    "        idx=torch.randperm(int(np.prod(dim)*uni_prob)).reshape(1,-1).to(device)\n",
    "        list(ind1.parameters())[i]=list(ind1.parameters())[i].reshape(1,-1).scatter_(1,idx,list(ind2.parameters())[i].reshape(1,-1)).reshape(dim)\n",
    "        list(ind2.parameters())[i]=list(ind2.parameters())[i].reshape(1,-1).scatter_(1,idx,list(ind1.parameters())[i].reshape(1,-1)).reshape(dim)\n",
    "    sample = rnd.random()\n",
    "    if sample<0.4:\n",
    "        ind1=mutate(ind1,mut_threshold)\n",
    "        ind2=mutate(ind2,mut_threshold)\n",
    "    offspring.extend((ind1,ind2))\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZXhu7oqLTuvk"
   },
   "outputs": [],
   "source": [
    "def return_mutation(agents, sorted_parent_indexes,n_elite,n_mut,mut_threshold):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    for i in range(n_mut-n_elite):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index],mut_threshold))\n",
    "        \n",
    "    for i in range(n_elite):\n",
    "        children_agents.append(agents[sorted_parent_indexes[i]])\n",
    "    return children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7O21xTaqTuvz"
   },
   "outputs": [],
   "source": [
    "def rank_selection(agents, sorted_parent_indexes,cross):\n",
    "    \"\"\"Select individuals for reproduction by giving preference to fittest individuals\n",
    "    \"\"\"\n",
    "    \n",
    "    children_agents = []\n",
    "\n",
    "    for i in range(cross):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "#         print(\"selected_agent_index\",selected_agent_index)\n",
    "        children_agents.append(agents[selected_agent_index])\n",
    "    return children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vxZMAZ805wNL",
    "outputId": "0505e569-bfc9-4ad2-dbcb-8a82bc4617a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation  0  | Mean rewards:  19.055333333333333  | Mean of top 5:  28.4\n",
      "Rewards for top:  [31.3, 28.2, 28.0, 27.6, 26.9, 26.6, 26.2, 25.8, 25.5, 25.4, 25.1, 24.9, 24.3, 24.0, 23.9, 23.8, 23.7, 23.5, 23.1, 22.9]\n",
      "\n",
      "\n",
      "Generation  1  | Mean rewards:  15.68733333333333  | Mean of top 5:  47.14000000000001\n",
      "Rewards for top:  [68.2, 47.2, 43.7, 40.9, 35.7, 32.6, 32.5, 29.1, 28.5, 27.6, 27.2, 26.2, 25.5, 25.5, 25.3, 25.0, 24.8, 24.2, 24.2, 24.1]\n",
      "\n",
      "\n",
      "Generation  2  | Mean rewards:  16.621333333333332  | Mean of top 5:  50.8\n",
      "Rewards for top:  [64.5, 48.8, 48.3, 47.2, 45.2, 45.0, 45.0, 43.6, 43.1, 42.5, 41.6, 41.3, 41.1, 37.8, 37.1, 35.9, 34.0, 32.0, 31.1, 30.1]\n",
      "\n",
      "\n",
      "Generation  3  | Mean rewards:  17.69533333333333  | Mean of top 5:  62.15999999999999\n",
      "Rewards for top:  [80.5, 71.0, 58.9, 51.0, 49.4, 48.8, 48.3, 47.6, 46.0, 45.7, 44.4, 37.5, 35.5, 35.5, 35.3, 32.3, 31.7, 30.7, 30.5, 30.3]\n",
      "\n",
      "\n",
      "Generation  4  | Mean rewards:  31.533999999999995  | Mean of top 5:  102.8\n",
      "Rewards for top:  [116.1, 111.9, 106.4, 93.0, 86.6, 86.5, 76.9, 73.3, 70.7, 68.1, 65.8, 63.3, 62.1, 59.2, 57.9, 57.9, 55.0, 54.8, 54.7, 54.6]\n",
      "\n",
      "\n",
      "Generation  5  | Mean rewards:  23.776  | Mean of top 5:  105.58\n",
      "Rewards for top:  [183.0, 101.0, 92.4, 86.3, 65.2, 58.9, 57.2, 56.3, 50.8, 46.6, 46.1, 45.4, 43.8, 42.6, 42.5, 42.4, 41.2, 40.8, 40.5, 40.1]\n",
      "\n",
      "\n",
      "Generation  6  | Mean rewards:  41.55533333333333  | Mean of top 5:  171.68\n",
      "Rewards for top:  [180.0, 177.5, 171.6, 168.5, 160.8, 159.6, 159.3, 157.4, 155.3, 154.1, 153.0, 151.8, 151.3, 149.9, 129.9, 106.2, 79.5, 77.6, 75.8, 68.8]\n",
      "\n",
      "\n",
      "Generation  7  | Mean rewards:  27.915333333333333  | Mean of top 5:  182.22\n",
      "Rewards for top:  [250.0, 202.3, 158.3, 150.6, 149.9, 147.1, 140.4, 131.3, 124.4, 119.8, 111.7, 105.5, 104.3, 84.8, 82.6, 75.3, 72.5, 53.8, 53.6, 49.7]\n",
      "\n",
      "\n",
      "Generation  8  | Mean rewards:  31.24  | Mean of top 5:  158.44\n",
      "Rewards for top:  [211.3, 153.3, 148.7, 148.2, 130.7, 130.3, 128.3, 100.1, 98.2, 93.1, 91.0, 90.8, 77.4, 74.8, 71.4, 70.7, 63.3, 62.1, 60.6, 55.6]\n",
      "\n",
      "\n",
      "Generation  9  | Mean rewards:  34.440666666666665  | Mean of top 5:  185.5\n",
      "Rewards for top:  [250.0, 226.6, 164.7, 151.1, 135.1, 132.4, 131.6, 122.9, 122.3, 118.6, 108.6, 108.2, 107.4, 104.5, 88.6, 76.8, 76.6, 72.7, 72.3, 71.1]\n",
      "\n",
      "\n",
      "Generation  10  | Mean rewards:  38.25200000000001  | Mean of top 5:  244.95999999999998\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 243.4, 231.4, 216.0, 213.4, 161.8, 138.7, 127.7, 120.5, 108.3, 107.6, 103.7, 98.1, 91.8, 91.6, 85.2, 82.0, 81.7]\n",
      "\n",
      "\n",
      "Generation  11  | Mean rewards:  38.507999999999996  | Mean of top 5:  212.92\n",
      "Rewards for top:  [250.0, 238.3, 208.3, 203.7, 164.3, 155.5, 144.5, 131.2, 128.1, 113.6, 113.5, 111.5, 109.8, 106.6, 103.6, 80.8, 78.0, 75.5, 73.5, 71.2]\n",
      "\n",
      "\n",
      "Generation  12  | Mean rewards:  68.93  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 242.7, 219.3, 216.3, 214.7, 180.6, 168.9, 165.6, 157.5, 157.4, 145.1, 139.2, 137.0, 135.0, 133.5, 133.2]\n",
      "\n",
      "\n",
      "Generation  13  | Mean rewards:  83.68533333333335  | Mean of top 5:  242.58\n",
      "Rewards for top:  [250.0, 245.7, 245.7, 239.0, 232.5, 232.3, 211.8, 199.1, 197.6, 192.9, 186.4, 185.8, 180.8, 172.5, 170.3, 168.3, 167.3, 166.2, 162.2, 161.7]\n",
      "\n",
      "\n",
      "Generation  14  | Mean rewards:  66.91933333333333  | Mean of top 5:  249.4\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 247.0, 243.4, 218.8, 205.0, 196.5, 193.0, 190.9, 174.4, 157.2, 156.4, 156.0, 152.7, 145.3, 144.1, 143.9, 141.9]\n",
      "\n",
      "\n",
      "Generation  15  | Mean rewards:  64.658  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 246.9, 240.4, 238.8, 233.4, 228.2, 226.2, 209.3, 206.8, 206.4, 206.2, 205.6, 199.4, 197.5]\n",
      "\n",
      "\n",
      "Generation  16  | Mean rewards:  64.90866666666668  | Mean of top 5:  245.34\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 240.1, 236.6, 228.5, 210.2, 196.4, 188.9, 180.6, 180.1, 166.8, 163.4, 163.3, 158.8, 157.2, 157.1, 155.6, 154.1, 153.6]\n",
      "\n",
      "\n",
      "Generation  17  | Mean rewards:  60.929333333333346  | Mean of top 5:  246.28000000000003\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 249.5, 231.9, 229.0, 228.7, 223.9, 194.3, 183.3, 158.8, 156.1, 155.2, 155.0, 152.2, 152.0, 151.8, 150.6, 146.5, 144.7]\n",
      "\n",
      "\n",
      "Generation  18  | Mean rewards:  96.49733333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.9, 241.5, 238.9, 237.9, 206.7, 202.1, 195.4, 190.7, 186.4, 182.2]\n",
      "\n",
      "\n",
      "Generation  19  | Mean rewards:  98.39666666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 248.9, 244.7, 242.6, 242.5, 241.1, 240.4, 239.3, 238.7, 238.4, 236.7, 235.5, 231.7]\n",
      "\n",
      "\n",
      "Generation  20  | Mean rewards:  95.35333333333334  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.1, 248.2, 248.0, 247.2, 246.2, 243.0, 239.5, 237.1, 235.3, 226.7]\n",
      "\n",
      "\n",
      "Generation  21  | Mean rewards:  125.55666666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  22  | Mean rewards:  106.92866666666669  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  23  | Mean rewards:  127.24866666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  24  | Mean rewards:  115.52333333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.6, 248.3, 243.0, 242.8, 242.7, 238.7, 238.0, 237.0, 235.6, 233.0, 227.3, 223.9, 223.6, 223.5]\n",
      "\n",
      "\n",
      "Generation  25  | Mean rewards:  125.62266666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  26  | Mean rewards:  41.58733333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 222.7, 209.7, 209.3, 206.5, 201.4, 186.5, 182.4, 177.6, 172.6, 163.6, 153.2, 153.0, 141.1]\n",
      "\n",
      "\n",
      "Generation  27  | Mean rewards:  166.71266666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  28  | Mean rewards:  139.99866666666665  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  29  | Mean rewards:  82.16333333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 246.9, 243.1, 243.0, 240.6, 239.4]\n",
      "\n",
      "\n",
      "Generation  30  | Mean rewards:  71.192  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 247.6, 246.8, 245.8, 244.8, 242.1, 234.8, 229.7, 227.3, 227.0, 225.9, 186.4, 178.0, 153.9, 153.9, 152.9]\n",
      "\n",
      "\n",
      "Generation  31  | Mean rewards:  141.32799999999997  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  32  | Mean rewards:  22.476666666666667  | Mean of top 5:  249.58\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 247.9, 136.8, 112.2, 105.5, 90.6, 89.6, 84.5, 82.4, 55.6, 50.2, 29.8, 19.7, 17.1, 16.9, 16.8, 16.5]\n",
      "\n",
      "\n",
      "Generation  33  | Mean rewards:  68.584  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.0, 247.9, 247.0, 243.7, 242.3, 241.7, 233.4, 226.9, 226.4, 218.0, 213.7]\n",
      "\n",
      "\n",
      "Generation  34  | Mean rewards:  98.51866666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  35  | Mean rewards:  40.022666666666666  | Mean of top 5:  249.71999999999997\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 248.6, 234.9, 222.9, 215.4, 212.9, 201.2, 198.5, 181.3, 143.0, 140.3, 140.0, 125.8, 124.1, 113.1, 104.6, 100.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation  36  | Mean rewards:  136.08466666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.8, 249.2]\n",
      "\n",
      "\n",
      "Generation  37  | Mean rewards:  116.76933333333335  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 242.8, 241.6, 241.5, 237.7, 235.6, 231.4]\n",
      "\n",
      "\n",
      "Generation  38  | Mean rewards:  93.13600000000001  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  39  | Mean rewards:  105.92333333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 248.2, 247.7, 247.0, 246.3, 244.6, 242.8, 242.7, 242.7, 242.0]\n",
      "\n",
      "\n",
      "Generation  40  | Mean rewards:  92.182  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.3, 249.0]\n",
      "\n",
      "\n",
      "Generation  41  | Mean rewards:  127.83933333333334  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  42  | Mean rewards:  115.97266666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  43  | Mean rewards:  122.50399999999999  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  44  | Mean rewards:  154.47733333333335  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  45  | Mean rewards:  113.93933333333334  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  46  | Mean rewards:  58.08066666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.3, 249.3, 249.0, 248.6, 247.7, 247.0, 246.9, 246.3, 227.5, 202.0, 191.4, 189.9, 174.9]\n",
      "\n",
      "\n",
      "Generation  47  | Mean rewards:  142.25199999999998  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  48  | Mean rewards:  127.53466666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  49  | Mean rewards:  130.8406666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 248.3, 248.1, 244.3]\n",
      "\n",
      "\n",
      "Generation  50  | Mean rewards:  133.74000000000004  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.1, 247.7]\n",
      "\n",
      "\n",
      "Generation  51  | Mean rewards:  76.468  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 247.7, 247.6, 247.4, 246.6, 245.0, 244.5, 244.1, 243.7, 221.9]\n",
      "\n",
      "\n",
      "Generation  52  | Mean rewards:  147.25733333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  53  | Mean rewards:  83.97466666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.7, 249.1, 247.6, 247.5]\n",
      "\n",
      "\n",
      "Generation  54  | Mean rewards:  64.65  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 248.5, 218.6, 207.3, 203.5, 188.9, 177.1, 152.7, 148.2, 146.2, 145.5, 138.3, 132.0, 130.1, 127.3, 126.0]\n",
      "\n",
      "\n",
      "Generation  55  | Mean rewards:  126.016  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  56  | Mean rewards:  83.202  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 249.8, 249.0]\n",
      "\n",
      "\n",
      "Generation  57  | Mean rewards:  187.91333333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  58  | Mean rewards:  153.74266666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  59  | Mean rewards:  147.28066666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  60  | Mean rewards:  128.444  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  61  | Mean rewards:  163.12666666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  62  | Mean rewards:  192.5386666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  63  | Mean rewards:  172.00133333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  64  | Mean rewards:  136.08866666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  65  | Mean rewards:  180.47666666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  66  | Mean rewards:  230.57200000000003  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  67  | Mean rewards:  202.12933333333334  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  68  | Mean rewards:  178.16333333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  69  | Mean rewards:  192.00733333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  70  | Mean rewards:  200.94733333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation  71  | Mean rewards:  222.25133333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  72  | Mean rewards:  226.088  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  73  | Mean rewards:  220.99  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  74  | Mean rewards:  215.15133333333333  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  75  | Mean rewards:  228.37466666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  76  | Mean rewards:  223.59266666666662  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  77  | Mean rewards:  191.35733333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  78  | Mean rewards:  181.84866666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  79  | Mean rewards:  197.91799999999998  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  80  | Mean rewards:  204.89733333333334  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  81  | Mean rewards:  161.12  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  82  | Mean rewards:  136.82200000000003  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  83  | Mean rewards:  156.584  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  84  | Mean rewards:  215.05733333333336  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  85  | Mean rewards:  193.48066666666665  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  86  | Mean rewards:  181.27866666666662  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  87  | Mean rewards:  184.28666666666666  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  88  | Mean rewards:  164.32333333333332  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  89  | Mean rewards:  176.36666666666667  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  90  | Mean rewards:  195.46  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  91  | Mean rewards:  191.09799999999998  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  92  | Mean rewards:  191.054  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  93  | Mean rewards:  195.30066666666664  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  94  | Mean rewards:  220.316  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  95  | Mean rewards:  201.01666666666668  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  96  | Mean rewards:  197.18599999999998  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  97  | Mean rewards:  210.67  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  98  | Mean rewards:  163.578  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "\n",
      "\n",
      "Generation  99  | Mean rewards:  218.112  | Mean of top 5:  250.0\n",
      "Rewards for top:  [250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0, 250.0]\n",
      "Solved after-1 episodes\n"
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 150\n",
    "n_elite=5\n",
    "\n",
    "agents = population(num_agents)\n",
    "# # How many top agents to consider as parents\n",
    "top_limit = 20\n",
    "\n",
    "# # # run evolution until X generations\n",
    "generations = 500\n",
    "\n",
    "#threshold decay for crossover\n",
    "EPS_START = 0.6\n",
    "EPS_END = 0.2\n",
    "\n",
    "steps_done=0\n",
    "\n",
    "#mutation threshold is exponentially decayed from 0.9 (higher exploration) to 0.01 in later generations\n",
    "MUT_START = 0.90\n",
    "MUT_END = 0.01\n",
    "EPS_DECAY = 200\n",
    "cross=round(num_agents*0.7/2)*2\n",
    "mut=num_agents-cross\n",
    "scores=[]\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    rewards=list(map(fitness,agents))\n",
    "    \n",
    "    #select top 20 individuals\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "    \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "    \n",
    "    mut_threshold = EPS_END + (MUT_START - MUT_END) * \\\n",
    "    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    #select children for breeding\n",
    "    mating=rank_selection(agents, sorted_parent_indexes,cross)\n",
    "    children=[]\n",
    "    for j in range(int(len(mating)/2)):\n",
    "        \n",
    "        #perform crossover of 70% population\n",
    "        child=uniform_crossover(mating[j],mating[len(mating)-j-1],4,0.2,eps_threshold)  \n",
    "        children.extend(child)\n",
    "\n",
    "    #only mutate 30% of the population\n",
    "    children_agents=return_mutation(agents, sorted_parent_indexes,n_elite,mut,mut_threshold) \n",
    "    children=children_agents+children\n",
    "  \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "\n",
    "    agents=children \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    scores.append(top_rewards[0])\n",
    "    if len(scores) >= 100:\n",
    "        if np.mean(scores[-100:]) >= 195.0:\n",
    "            print('Solved after' + str(generation-100) + ' episodes')\n",
    "            break\n",
    "\n",
    "    steps_done+=1\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot rewards for the top agent during training for each episode\n",
    "plt.plot(range(len(scores)),scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6SlMJiXv0Jk6"
   },
   "outputs": [],
   "source": [
    "#select one of the top performing agent\n",
    "rewards=list(map(fitness,agents))\n",
    "sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit]\n",
    "top_agent=agents[sorted_parent_indexes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YjE-qJvk0tbg"
   },
   "outputs": [],
   "source": [
    "#creating single environment for testing\n",
    "env_1= gym.make(env_name)\n",
    "\n",
    "#Play a random game\n",
    "def plot_reward(n,agent):\n",
    "    for i in range(n):\n",
    "        state = env_1.reset()\n",
    "        r=0\n",
    "        done = False\n",
    "        while not done:\n",
    "          state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "          output_probabilities = agent(state)#.cpu().numpy()[0]\n",
    "          action=output_probabilities.multinomial(1).squeeze(1).cpu().numpy()[0]\n",
    "          state, reward, done, _ = env_1.step(action)\n",
    "          r=r+reward\n",
    "        print(f\"episode {i} reward {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dD3WcWW3Tuwh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward 316.0\n",
      "episode 1 reward 285.0\n",
      "episode 2 reward 319.0\n",
      "episode 3 reward 289.0\n",
      "episode 4 reward 343.0\n",
      "episode 5 reward 343.0\n",
      "episode 6 reward 306.0\n",
      "episode 7 reward 347.0\n",
      "episode 8 reward 315.0\n",
      "episode 9 reward 325.0\n"
     ]
    }
   ],
   "source": [
    "#reward for top agent after training\n",
    "plot_reward(10,top_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
